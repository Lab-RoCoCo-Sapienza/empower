<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Multi-agent Planning using Visual Language Models">
  <meta name="keywords"
    content="Multi-agent Planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>


</head>

<body>
   <style>
        .containeritem {
            display: flex;
            flex-wrap: wrap;
        }
        .column {
            flex: 33.33%;
            padding: 10px;
        }
        ul {
            list-style-type: none;
        }
        li {
            margin: 5px 0;
        }
    </style>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Related Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://lab-rococo-sapienza.github.io/map-vlm/">
              ECAI
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img style="height:6vw; margin-bottom: 3px;" src ="assets/images/logo.png">
            <h1 class="title is-2 publication-title">Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution</h1>
            <h2 class="title is-6 publlication-title">IROS 2024</h2>
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/fra-arg/"><i>Francesco Argenziano</i></a> <sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/michele-brienza-ba0292253/"><i>Michele Brienza</i></a> <sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/vincenzo-suriani-549429127/"><i>Vincenzo Suriani </i></a> <sup>2</sup>,
              </span>
              <span class="author-block">
                <a href=""><i>Daniele Nardi</i></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://web.unibas.it/bloisi/"><i>Domenico Daniele Bloisi</i></a> <sup>3</sup>,
              </span>
              <br>
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>Sapienza University of Rome</span>
              <span class="author-block"><sup>2</sup>University of Basilicata</span>
              <span class="author-block"><sup>3</sup>International University of Rome</span>
              <br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="." class="external-link button is-normal ">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2408.17379" class="external-link button is-normal ">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/Lab-RoCoCo-Sapienza/empower/tree/main" class="external-link button is-normal ">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="container is-max-desktop">
      <img src="assets/images/iros_methodology.png" alt="Architecture" style="width:150%;">
      <h5 class="has-text-justified">Complete architecture of EMPOWER, from the task description to the execution of the plan in the world. The RGB image is used to extract a graph of the scene as long as the final plan and the object labels. These labels are then grounded via an NLP pipeline and reprojected onto the pointclouds extracted from the depth image of the robot. Lastly, reference points are computed from these pointclouds to facilitate actions in the world. Use case illustrated: order the objects on the table from the highest to the lowest.</h5>
    </div>

  <div style = "margin-top: 3%" class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Abstract</h2>
      <div class="column has-text-justified">
        <p>

          Task planning for robots in real-life settings presents significant challenges. These challenges stem from three primary issues: the difficulty in identifying grounded sequences of steps to achieve a goal; the lack of a standardized mapping between high-level actions and low-level commands; and the challenge of maintaining low computational overhead given the limited resources of robotic hardware.
          We introduce EMPOWER, a framework designed for open-vocabulary online grounding and planning for embodied agents aimed at addressing these issues. By leveraging efficient pre-trained foundation models and a multi-role mechanism, EMPOWER demonstrates notable improvements in grounded planning and execution.
          Quantitative results highlight the effectiveness of our approach, achieving an average success rate of 0.73 across six different real-life scenarios using a TIAGo robot. 
        </p>
      </div>


    </div>

  </div>

  <div style="width:60%;" class="video_item">
    <video controls>
        <source src="assets/videos/output_video.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>
</div>


  <div style = "margin-top: 3%" class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Approach</h2>
      <div class="column has-text-justified">
        <p>Our work consists of three components: a planner that generates a plan from a task description with a multi-role architecture; a model that grounds the plan in the environment 
          using NLP techniques and an executor that executes the plan on a robot mapping the high-level actions to low-level commands. Thanks to the use of 
          The contributions of our work are the following:
            <li> We propose a multi-role, single-body hierarchical subtask division that enables complex reasoning about the
              scene and allows for the creation of plans that can be grounded in the environment;</li>
            <li>We present a complete pipeline that incorporates FMs
              for extracting meaningful information from the world,
              while being resilient to the online constraint given by
              the limited computational resources of real robots;</li>
            <li>We provide a flexible solution that, thanks to our
              novel open-vocabulary framework, grounds and deploys plans across different robotic platforms in the
              environment. </li>
            Our approach enables planning for embodied agents in an open vocabulary setting through NLP-driven semantic comprehension of scene elements. We first assess the improvement in high-level planning by comparing our multi-role VLM-based architecture to a single-role approach.
            Subsequently, we evaluate the system's full planning capabilities, from high to low levels, by executing tasks with a TIAGo robot in real environments and measuring the success rate.
        </p>
      </div>

    </div>
  </div>

  <div style = "margin-top: 3%" class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Experiments</h2>
      <div class="column has-text-justified">
        <p>Our tests focus on six representative use cases that we have designed to test the planning capabilities of the LLMs for indoor tasks that require multi-step reasoning and manipulations.
           These cases are: sort object by their height, grab a jacket on the coat rack, throw the objects in the right recycle bins, order the shelf to have 2 objects per level,
            order the shelf depending on the objects’ material, exit the room.


        </p>    <div class="containeritem">
          <div class="column">
              <ul>
                  <li><a href="experiments/recycle.html">recycle the objects</a></li>
                  <li><a href="experiments/height_order.html">order the objects by height</a></li>
                  <li><a href="experiments/jacket.html">give me the jacket</a></li>
              </ul>
          </div>
          <div class="column">
              <ul>
                  <li><a href="experiments/order_shelf.html">order the shelf by number of objects</a></li>
                  <li><a href="experiments/order_material.html">order the shelf by objects' material</a></li>
                  <li><a href="experiments/exit.html">exit the room</a></li>
              </ul>
          </div>
      </div>



  </section>
  
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{argenziano2024empowerembodiedmultiroleopenvocabulary,
        title={EMPOWER: Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution}, 
        author={Francesco Argenziano and Michele Brienza and Vincenzo Suriani and Daniele Nardi and Domenico D. Bloisi},
        year={2024},
        eprint={2408.17379},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        url={https://arxiv.org/abs/2408.17379}, }</code></pre>
    </div>
  </section>



  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
          <img alt="Creative Commons License" style="border-width:0"
            src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website adapted from the <a rel="license"
              href="https://github.com/concept-graphs/concept-graphs.github.io">Concept-Graph</a> template, which is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
              International License</a>. The template uses the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies
              source code</a>.
            </p>
          </div>
        </div>
      </div>
    </div>

  </footer>

</body>

</html>
